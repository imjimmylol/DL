save_dir: "./results/pong_attempt2"
wandb_run_name: "pong-aggressive-1"

batch_size: 32        # normal, no need to go too small. 32 is a standard good balance
memory_size: 500000   # enough but not too large, prioritize faster rotation
lr: 0.0000625         # slightly lower lr (~6.25e-5), more stable for sparse reward tasks

discount_factor: 0.99 # keep good, rewards are sparse and delayed

epsilon_start: 1.0    
epsilon_decay: 0.999995   # **much slower decay** (you need exploration longer in Pong)
epsilon_min: 0.05         # slightly lower minimum epsilon for late exploitation

target_update_frequency: 4000  # much **slower target net update** (stable targets are key)

replay_start_size: 50000  # **start training later**, let agent gather more experience first

max_episode_steps: 10000  # Pong can have long rallies, set a big enough cap

train_per_step: 1         # normal, 1 training step per env step

total_steps: 5000000      # you need much longer training (~5M steps minimum for Pong)
eval_frequency: 10000     # don't evaluate too frequently, evaluation can be noisy
eval_episodes: 10         # evaluate over more episodes to average randomness
