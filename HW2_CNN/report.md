
# å ±å‘Šç°¡å ±æ¨¡æ¿

## 1. Implementation Details (30%)
### 1.1 Model Specification and Structure
- **UNet**  
  - Follows the original design by the authors, without adding any BatchNormalization layers.  
  - Unlike other common Unet variants, after calculating dimensions as described in the original paper, no padding is used in the downscaling layers.  
  - During upsampling, to preserve the original data at the encoder's edges, no cropping is performed; instead, zero-padding is applied to the upsampled data to ensure the dimensions match.

- **RES(34)UNet**  
  - Adopts the same encoder-decoder architecture as the original design.  
  - Utilizes the Residual block design from ResNet34, without using the bottleneck design.  
  - In the decoder, in addition to the standard Unet upsampling, it adds a sequence of convolution, ReLU, batch normalization, and CBAM.




### 1.2 Training Logic & Eval

- Training Flow Chart: 

    ```mermaid
    graph TD
        A["Start Training"] --> B["Load Datasets (Train & Validation)"]
        B --> C["Create DataLoaders"]
        C --> D{"Checkpoint Provided?"}
        D -- Yes --> E["Load Checkpoint"]
        D -- No --> F["Initialize Model"]
        E --> F
        F --> G["Define Loss & Optimizer"]
        G --> H["Train Loop"]
        H --> I["Validation Loop"]
        I --> J["Log & Save Checkpoint"]
        J --> K{"Is Validation Loss Best?"}
        K -- Yes --> L["Save Best Model"]
        K -- No --> M["Continue Training"]
        L --> M
        M --> N["End Training"]
    ```

- Training Settings:
    - `--resume` : resume the assigned checkpoint
    - `--runame` : To specify run name for tensorboard visulization comparison with different model
    - if `resume` is assigned, change run name i

### é‡è¦è¨­è¨ˆé¸æ“‡èˆ‡ç†ç”±
- Loss functionã€optimizerã€learning rate ç­‰è¨­å®š
- æ¨¡çµ„ä¹‹é–“å¦‚ä½•å”ä½œ

---

## 2. Data Preprocessing (25%)
### åŸå§‹è³‡æ–™ä»‹ç´¹
- è³‡æ–™ä¾†æºèˆ‡åŸºæœ¬çµ±è¨ˆè³‡è¨Š
- è³‡æ–™ç•°å¸¸æˆ–ç¼ºå¤±æƒ…æ³èªªæ˜

### è³‡æ–™æ¸…ç†èˆ‡å¢å¼·æ–¹æ³•
- ä½¿ç”¨çš„æ¸…ç†ã€è½‰æ›ã€å¢å¼·æŠ€è¡“ï¼ˆä¾‹ï¼šNormalizationã€Data Augmentationï¼‰

### ç‰¹åˆ¥ä¹‹è™• / å‰µæ–°é»
- èˆ‡æ¨™æº–æ–¹æ³•çš„å·®ç•°
- é¡å¤–åˆ†ææˆ– insight

---

## 3. Analyze the Experiment Results (25%)
### è¶…åƒæ•¸èˆ‡è¨“ç·´ç­–ç•¥æ¢ç´¢
- æ¯”è¼ƒä¸åŒè¶…åƒæ•¸å°çµæœçš„å½±éŸ¿
- æ¨¡å‹é…ç½®èˆ‡ç­–ç•¥å¯¦é©—æ¯”è¼ƒï¼ˆå¯ç”¨è¡¨æ ¼æˆ–åœ–ï¼‰

### å¯¦é©—çµæœåˆ†æ
- Training vs Validation æ›²ç·šï¼ˆLoss / Accuracyï¼‰
- å¯è¦–åŒ–çµæœï¼ˆå¦‚æœæœ‰åœ–åƒä»»å‹™ï¼‰

### åˆ†æèˆ‡æ´å¯Ÿ
- æ•¸æ“šç‰¹æ€§è§€å¯Ÿèˆ‡æ¨è«–
- æœ‰å“ªäº›é‡è¦çµæœå½±éŸ¿äº†æœ€çµ‚é¸æ“‡

---

## 4. Execution Steps (0%)
### åŸ·è¡ŒæŒ‡ä»¤èˆ‡æµç¨‹
- è¨“ç·´æ¨¡å‹ç”¨çš„ command line æŒ‡ä»¤
- Inference æŒ‡ä»¤èˆ‡æµç¨‹

### åƒæ•¸è¨­å®š
- å¯¦éš›ä½¿ç”¨çš„åƒæ•¸è¨­å®šåˆ—è¡¨ï¼ˆè¡¨æ ¼æˆ– code snippetï¼‰

---

## Q&A / References
- æœ‰å•é¡Œæ­¡è¿ç™¼å• ğŸ™‹
- åƒè€ƒæ–‡ç»æˆ–å¤–éƒ¨è³‡æºåˆ—è¡¨ï¼ˆå¦‚æœ‰ï¼‰
